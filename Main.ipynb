{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c0ebbd-d889-4574-b450-280dae1f94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer,GPT2Config, GPT2Model\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "import gc \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82265c91-1406-41ec-bf62-674856852c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 10 16:45:02 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    Off | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   60C    P8              15W / 115W |     59MiB /  6144MiB |     40%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1986      G   /usr/lib/xorg/Xorg                           55MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd84a56-a2e5-4de3-b6ab-6be7259ba3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a3479-a41a-435e-875d-481d9141f15f",
   "metadata": {},
   "source": [
    "### Tokenizer/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f690f59a-9470-46c7-8d0e-0ff4c57b0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a52f62-ba3f-4404-a095-a9293f33870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2-medium', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437c780f-0705-45f1-88bb-0b3781574ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
    "                                \"bos_token\": \"<sos>\",\n",
    "                                \"eos_token\": \"<eos>\"})\n",
    "tokenizer.add_tokens([\"<bot>\",'<per>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d06e13-9a6b-4eab-b647-5fc78636bbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50262, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50262, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59299868-84fe-41cb-8bfc-f20290f72ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354828288"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748941a5-5206-4658-975d-2572e21c19b3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d000244-4ac6-4888-8910-6c34544fe80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f02fa44-4cd2-4f6c-b1de-bfce2b7dc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatData(Dataset):\n",
    "    def __init__(self, path:str, tokenizer):\n",
    "        self.file = json.load(open(path, \"r\"))\n",
    "\n",
    "        \n",
    "        self.data = [] # To store the dialogue \n",
    "\n",
    "        # Reading the dialogues \n",
    "        for idx, dialog in enumerate(self.file):\n",
    "            self.data.append([]) # new dialogue\n",
    "            for txt in dialog['dialog']:\n",
    "                self.data[idx].append(txt['text'])\n",
    "        # Formating the conversations \n",
    "        self.X = []\n",
    "        int1 = ''\n",
    "        int2 = ''\n",
    "        for idx, dialog in enumerate(self.data):\n",
    "            if len(dialog)==0:\n",
    "                continue\n",
    "            if len(dialog) % 2 == 0:\n",
    "                int1 = '<per>'\n",
    "                int2 = '<bot>'\n",
    "            else:\n",
    "                int1 = '<bot>'\n",
    "                int2 = '<per>'\n",
    "            tmp_str = '<sos>'\n",
    "            for k,utt in enumerate(dialog):\n",
    "                if k%2 == 0:\n",
    "                    tmp_str += int1 + utt\n",
    "                else:\n",
    "                    tmp_str += int2 + utt\n",
    "            tmp_str += '<eos>'\n",
    "            # print(len(tmp_str))\n",
    "            self.X.append(tmp_str)\n",
    "\n",
    "        # max = 0\n",
    "        # for dialog in self.X:\n",
    "        #     if len(dialog)>max:\n",
    "        #         max= len(dialog)\n",
    "        # print(max)\n",
    "    \n",
    "        \n",
    "        # Tokenization of the dataset \n",
    "        self.X_encoded = tokenizer(self.X,max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041807a4-0304-479c-9047-3cd2aa705f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatData = ChatData(\"./chat_data.json\", tokenizer)\n",
    "chatData =  DataLoader(chatData, batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f326792-3b62-41e0-b1da-dfd430e6fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = next(iter(chatData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6baa9aec-1adb-4fee-ad44-6056dfa54b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<sos> <bot> i am not sure what that is. <per> What are you not sure of? <bot> i am not sure. i am a very experienced person. <per> I bet you are. Tell me more about yourself <bot> i am a very experienced, but i would rather be a librarian. <per> Why librarian? <bot> i am not sure. i am sure they are. <per> You're sure they are what? <bot> i think they are fda regulated. <per> Librarians are fda regulated? <bot> no i have not. but i have heard that before. <per> Do you know what is fda? <bot> i think i am fda regulated. <per> What is fda? <bot> i think so. i think i will add some of my food to eat. <per> What do you like to eat? <bot> i do not think i have voluntarily tried it <per> You don't eat at all? <bot> i think i am available for that. <per> Do you drink! <bot> i am not a very good person. i am not very good at it. <per> You're not good at drinking? Why? <bot> i think i am not sure. i am sure it will. <per> It will what? <bot> i am not sure. i am a very experienced person. <per> You said that before. I know <bot> how many do you have? <per> How many what do I have? <bot> i have not tried that, but i have to try. <per> Try what? <bot> i am not sure. i am a very experienced person. <per> What are you experienced in? <bot> i am not sure, i have been looking for a new job. <per> What do you want to be? <bot> i am not sure. i am a very experienced person. <per> You should know that <bot> i think i am going to be a lawyer. <per> That's good. <bot> i think i will stick to that. <per> Yeah, sure <bot> i think i am going to be a doctor like my uncle. <per> But you said you're going to be a lawyer <bot> i can not say that i am not sure. i am a very experienced person. <per> Okay. <bot> i am not sure what that means. i am trying to get out more. <per> Let's go motorbiking with me. <bot> i will be using that app <per> That's not an app <bot> i can not say that. i am more of a loner. <per> Then let's have coffee. I love coffee. <bot> i will have to try that. what else do you enjoy? <per> I\",\n",
       " '<sos> <bot> Why? Because I got the kids three days a week and if I had a fulltime job...<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(ex[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d06cc1-6fe9-43d2-b056-ea2dbbaf449f",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516f5379-c1f2-44fb-ae8b-228812e96463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(chatData, model, optim,epochs=5):\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        for X, a in tqdm.tqdm(chatData):\n",
    "            garbage_collect()\n",
    "            X = X.to(device)\n",
    "            a = a.to(device)\n",
    "            # Ignoring the padding token by setting its label to-100\n",
    "            labels = X.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = model(X, attention_mask=a, labels=labels).loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss.detach()\n",
    "        print(f'Epoch {i}, Loss = {loss.item()}')\n",
    "        torch.save(model.state_dict(), \"model_state.pt\")\n",
    "        # print(infer(\"<sos> <per>hello how are you<bot>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf6a01f-9cad-40c3-b453-661dd3a9fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp):\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(X, attention_mask=a )\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c609d20-c5ff-4728-9372-23dd8a99f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9de38f7-3028-4b80-890c-768fb351ba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [01:19<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 4.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [01:19<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [01:17<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss = 3.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [01:18<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss = 1.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [01:17<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss = 2.203125\n"
     ]
    }
   ],
   "source": [
    "print(\"training .... \")\n",
    "train(chatData, model, optim,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fccadfc-7508-4008-9e55-661a77f11504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garbage_collect():\n",
    "    \"\"\"\n",
    "        Clear the memory of the GPU and CPU \n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5af18576-d78c-4e23-b766-50f192fd6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b3374-f963-4f34-8523-8be16c66d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  inp = input()\n",
    "  print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "539019e3-4b33-48dc-aad5-bc3e1d822c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>  <per> hello how are you <bot> I am a new person. <bot> hello <bot>  how are you\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"<sos> <per>hello how are you<bot>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3dca09-e87b-4dfb-bfda-7ac2008c8739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
